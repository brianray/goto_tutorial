{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doing more with data\n",
    "\n",
    "  1. download the file\n",
    "  2. unzip it\n",
    "  3. read it and convert to pandas dataframe\n",
    "  4. build a machine learning prediction engine\n",
    "\n",
    "\n",
    "Download link: https://www.dropbox.com/s/u6rtu7a7clb4k3x/yelp_training_set_review.json.zip?dl=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import humanize\n",
    "from __future__ import print_function\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "from StringIO import StringIO\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "humanize.naturalsize(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_url = \"https://www.dropbox.com/s/u6rtu7a7clb4k3x/yelp_training_set_review.json.zip?dl=1\"\n",
    "r = requests.get(file_url, stream=True)\n",
    "out = \"yelp_training_set_review.json.zip\"\n",
    "out_size = 0\n",
    "with open(out, \"wb\") as f:\n",
    "    chunk_size = 10000000\n",
    "    for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "        if chunk:\n",
    "            out_size += chunk_size\n",
    "            print(\"wrote {}\".format(humanize.naturalsize(out_size)))\n",
    "            f.write(chunk)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = \"yelp_training_set_review.json.zip\"\n",
    "zipf = zipfile.ZipFile(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zipf.printdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jsons = []\n",
    "[jsons.append(x) for x in zipf.read('yelp_training_set_review.json').split(\"\\n\")[:20000]]\n",
    "len(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_str = \"[{}]\".format(\",\".join(jsons))\n",
    "df = pd.read_json(json_str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for classtype in ('funny', 'useful', 'cool'):\n",
    "    df[classtype] = map(lambda votes: votes[classtype], df['votes'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop(['business_id', 'review_id', 'type', 'user_id', 'votes'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.barplot(x=df.stars, y=df.useful+df.cool+df.funny, color=\"red\")\n",
    "sns.barplot(x=df.stars, y=df.cool+df.funny, color=\"yellow\")\n",
    "sns.barplot(x=df.stars, y=df.funny, color=\"blue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_funny = df.loc[df['funny'] != 0]\n",
    "df_not_funny = df.loc[df['funny'] == 0]\n",
    "df_funny['funny'] = True\n",
    "df_not_funny['funny'] = False\n",
    "\n",
    "df_half_funny = pd.concat([df_funny, df_not_funny])\n",
    "df_half_funny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train, test, labels_train, labels_test = train_test_split(df_half_funny['text'].values,\n",
    "                                                          df_half_funny['funny'].values,\n",
    "                                                          test_size=0.20,\n",
    "                                                          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorize', CountVectorizer(ngram_range=(1, 3))),\n",
    "    ('tfidf_trandformer', TfidfTransformer()),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(train, labels_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.score(test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred_label_test = pipeline.predict(test)\n",
    "confusion_matrix(labels_test, pred_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.predict_proba([\"I went to the cheese shop and there wasn't any cheese\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = pipeline.predict_proba(test)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(labels_test, [x[0]*-1 for x in scores])\n",
    "fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt = pyplot\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"Roc curve (area = %0.2f)\" % auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"false Positive Rate\")\n",
    "plt.ylabel(\"true Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Your own\n",
    "\n",
    "Do the same thing with the NHTSA. Consider building a prediction based off if the report was labeled with a injury. There is an injury column in the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.dropbox.com/s/fjwu5iassqtsnyo/nhtsa_as_xml.zip?dl=1\")\n",
    "zipf = zipfile.ZipFile(StringIO(r.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "injured_file = zipf.read(\"nhtsa_injured.xml\")\n",
    "not_injured_file = zipf.read(\"nhtsa_not_injured.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itree = etree.parse(StringIO(injured_file))\n",
    "nitree = etree.parse(StringIO(not_injured_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = zip(itree.xpath(\"//rows/row/@c2\"), itree.xpath(\"//rows/row/@c8\")) + zip(nitree.xpath(\"//rows/row/@c2\"), nitree.xpath(\"//rows/row/@c8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
